{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation-Maximization algorithm from scratch for recommendations via matrix completion\n",
    "\n",
    "First, we provide some background on the algorithms being used in this project. Then, we build the Expectation-Maximization algorithm from scratch using only numpy. We use our algorithm to construct a Gaussian mixture model which we use to fill an incomplete rating matrix for the purpose of providing recommendations. Finally, we compare our model with the most popular clustering model provided by sklearn: sklearn.clustering.KMeans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means clustering algorithm\n",
    "\n",
    "Given the data points $x_1,\\ldots,x_n$, $k$-means clustering partitions the $n$ points into $k\\leq n$ parts $P_1,\\ldots, P_k$ so that the sum of the variance with each cluster is minimal. The algorithm is an iterative process with each iteration consisting of two steps. \n",
    "\n",
    "#### Assignment Step\n",
    "We calculate the distance between each point $x_i$ and the mean of each cluster $P_j$ where $i=1,\\ldots,n$ and $j=1,\\ldots, k$. Then we assign each point to the nearest cluster. In particular, we use the Euclidean distance.\n",
    "$$\n",
    "d(x,y)=(\\sum_{i=1}^d |x_i-y_i|)^\\frac{1}{2}\n",
    "$$\n",
    "Here $x$ and $y$ are samples with $d$ dimensions and $x_i$ and $y_i$ are the $i$th dimension of each.\n",
    "\n",
    "#### Update Step\n",
    "We recalculate all the means for each cluster.\n",
    "$$\n",
    "\\mu_j = \\frac{1}{n_j}\\sum_{i=1}^{n_j} x_i\n",
    "$$\n",
    "$\\mu_j$ is the mean for cluster $P_j$ and $n_j=|P_j|$.\n",
    "\n",
    "#### Initialization\n",
    "We initialize the means by randomly choosing $k$ distinct samples from the data and using them as the initial $k$ means. We choose this method compared to others (like randomly assigning each point to a part) because it has a higher probability of spreading the intial means out in comparison.\n",
    "\n",
    "### Considerations\n",
    "Another popular metric is derived from cosine similarity. However, cosine similarity ignores magnitude which is very important in this case. For example, the points $(1,1)$ and $(5,5)$ would have a maximal cosine similarity of $1$ but are completely different in terms of ratings. Hence in this case, Euclidean is the better choice.\n",
    "\n",
    "## Mixture models\n",
    "A mixture model can be thought of as a generalization of the $k$-means clustering algorithm. In particular, we will use the Gaussian mixture model. This means that we are assuming that the each sample is a linear combination of Gaussian distributions.\n",
    "\n",
    "#### Log-likelihood function\n",
    "Instead of measuring the distance between sample $x_i$ and the mean $\\mu_j$ of cluster $P_j$, we consider the probability or $\\textit{likelihood}$ that $x_i$ belongs to $P_j$. For each cluster $P_j$ we still have the mean $\\mu_j$. However, we also have the variance $\\sigma^2_j$ as well as probabilities or $\\textit{weights}$ $\\pi_j$. The likelihood function $L(X,\\Theta)$ is the probability that the instance $X$ occurs given the current $k$ distributions $\\Theta$. Recall that in the $k$-means clustering algorithm, our goal was to minimized the total error which we computed as the sum of the variances of each cluster. Analogously, for our mixture model we want to maximize our likelihood function. In other words we want to find the parameters $\\mu_j, \\sigma^2_j, \\pi_j$ which maximizes the chance that our data was generated by \n",
    "$$\n",
    "\\sum_{j=1}^k \\pi_j N(\\mu_j,\\sigma_j).\n",
    "$$\n",
    "The probability that this holds for all samples in $X$ is the product of the individual probabilities. However, it is more convenient to equivalently maximize the log $\\log(L(X,\\Theta))$. Since $\\log(ab)=\\log(a)+\\log(b)$ we get the following log-likelihood function:\n",
    "$$\n",
    "\\ell(X, \\Theta) = \\sum_{i=1}^{n} \\log \\left( \\sum_{j=1}^{k} \\pi_j \\mathcal{N}(x_{C_i}^{(i)} | \\mu_{C_i}^{(j)}, \\sigma_j^2I_{d \\times d}) \\right)\n",
    "$$\n",
    "where $x$ is our dataset, $\\Theta$ is our mixture model, $k$ is the number of clusters, $C_i$ is a $d$-dimensional indicator vector or $\\textit{one-hot encoding}$ of the movies rated by customer $i$, and \n",
    "$$\n",
    "\\mathcal{N}(x| \\mu, \\sigma^2I_{d\\times d}) = \\frac{1}{(2\\pi \\sigma^2)^{\\frac{d}{2}}} \\exp\\left( -\\frac{1}{2\\sigma^2} \\|x - \\mu \\|^2 \\right)\n",
    "$$\n",
    "\n",
    "### EM algorithm\n",
    "Normally in statistical analysis, we would use the MLE or $\\textit{maximum likelihood estimator}$ for estimating the distribution $\\Theta$. However, as is the reason for using gradient descent, we must find an alternative which is reasonable in terms of computational complexity. In particular, we use the EM algorithm or $\\textit{expectation maximization algorithm}$. The EM algorithm is an iterative algorithm and each iteration consists of two steps.\n",
    "\n",
    "#### Expectation step\n",
    "For each sample $x_i$, the algorithm assigns $x_i$ a weight $\\pi_j$ for each $j=1,\\ldots,k$. This is analogous to assigning each $x_i$ to a cluster in the $k$-means clustering algorithm except that now its assignment is in the form of a random variable with parameters $\\pi_1,\\ldots, \\pi_k$. Using Bayes' theorem, we get the following assignment.\n",
    "$$\n",
    "P(j|i) = \\frac{\\pi_j \\mathcal{N}(x_{C_i}^{(i)} | \\mu_{C_i}^{(j)}, \\sigma_j^2I_{d \\times d})}{\\sum_{j=1}^{k} \\pi_j \\mathcal{N}(x_{C_i}^{(i)} | \\mu_{C_i}^{(j)}, \\sigma_j^2I_{d \\times d}) }\n",
    "$$\n",
    "\n",
    "#### Maximization step\n",
    "The algorithm then updates the parameters as we did in $k$-means.\n",
    "$$\n",
    "\\hat{\\pi}_j = \\frac{\\sum_{i=1}^{n} P(j|i)}{n}\n",
    "$$\n",
    "$$\n",
    "\\hat{\\mu}^{(j)}_l = \\frac{\\sum_{i=1}^{n} \\delta_{C_i}(l) P(j|i)x^{(i)}_l}{\\sum_{i=1}^{n} \\delta_{C_i}(l)P(j|i)}\n",
    "$$\n",
    "$$\n",
    "\\hat{\\sigma}^2_j = \\frac{\\sum_{i=1}^{n}P(j|i) \\|x^{(i)}_{C_i} - \\hat{i}^{(j)}_{C_i} \\|^2}{\\sum_{i=1}^{n} d P(j|i)}\n",
    "$$\n",
    "Here $j,l=1,\\ldots,k$ and $\\delta_{C_i}$ is the indicator function of $C_i$.\n",
    "\n",
    "#### Connection remark\n",
    "If the covariance matrix in the EM algorithm is a constant multiple of an identity matrix, then it is equivalent to the $k$-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Expectation Maximization algorithm from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans #for comparative analysis\n",
    "import matplotlib.pyplot as plt #for plotting the log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "First we import our partially observed data matrix X along with the complete matrix which we will use as the ground truth to compare to later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt('netflix_incomplete.txt')\n",
    "X_complete = np.loadtxt('netflix_complete.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract some fixed parameters from our data for convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n: number of users, d: number of movies\n",
    "n, d = X.shape\n",
    "\n",
    "# K: number of clusters/Gaussians\n",
    "K = 12\n",
    "\n",
    "# delta: matrix indicating missing reviews\n",
    "delta = np.where(X == 0,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Expectation Maximization (EM) Algorithm\n",
    "\n",
    "First we initialize the parameters of our mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu: d-dimensional mean for each Gaussian\n",
    "np.random.seed(1)\n",
    "mu = X[np.random.choice(n, K, replace = False)]\n",
    "\n",
    "# p: weights for each Gaussian\n",
    "p = np.ones(K)/K\n",
    "\n",
    "# var: 1-dimensional variance for each Gaussian\n",
    "var = np.sum((mu*np.ones([n,K,d]) - X.reshape([n,1,d]))**2, axis=(0,2))/(n*d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for computing the squared norm which depends only on the current mu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_norm(mu):\n",
    "    U = (mu*np.ones([n,K,d]))*delta.reshape([n,1,d])\n",
    "    sub_stack = U - X.reshape([n,1,d])\n",
    "    return np.sum(sub_stack**2, axis = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectation-step\n",
    "\n",
    "We define a function which runs the E-Step of the EM algorithm returning the soft counts (posterior) and the log-likelihood of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estep(X,mu,p,var):\n",
    "    \n",
    "    norm = compute_norm(mu)\n",
    "    \n",
    "    C_u = np.sum(delta,axis=1,keepdims=True)\n",
    "    logged_gauss = np.log(p) - C_u/2*np.log(2*np.pi*var*np.ones([n,K])) - norm/(2*var)\n",
    "    max_vector = np.amax(logged_gauss, axis=1, keepdims=True)\n",
    "    scaled_gauss = np.exp(logged_gauss - max_vector)\n",
    "    denom = max_vector + np.log(np.sum(scaled_gauss, axis=1, keepdims=True))\n",
    "    \n",
    "    log_post = logged_gauss - denom\n",
    "    log_likelihood = np.sum(denom)\n",
    "    \n",
    "    return np.exp(log_post), log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximization-step\n",
    "\n",
    "We define a function which runs the M-Step of the EM algorithm returning the updated mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mstep(X, post, min_var, mu, p, var):\n",
    "    \n",
    "    norm = compute_norm(mu)\n",
    "    \n",
    "    #update mu\n",
    "    mu_numer = np.dot(X.T, post).T\n",
    "    mu_denom = np.dot(delta.T, post).T\n",
    "    mu = np.where(mu_denom >= 1, mu_numer/(mu_denom +1e-10), mu)\n",
    "    \n",
    "    #update var\n",
    "    C_u = np.sum(delta, axis=1, keepdims=True)\n",
    "    sum_factor = np.sum(post*norm, axis = 0)\n",
    "    first_factor = 1/np.sum(C_u*post, axis = 0)\n",
    "    var_bad = first_factor*sum_factor\n",
    "    var = np.where(var_bad < min_var, min_var, var_bad)\n",
    "    \n",
    "    #update p\n",
    "    p = np.sum(post, axis = 0)/n\n",
    "    \n",
    "    #update norm\n",
    "    norm = compute_norm(mu)\n",
    "    \n",
    "    return mu,p,var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the EM Algorithm\n",
    "\n",
    "We define a function which runs the entire EM algorithm using our E-step and M-step functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(X, mu, p ,var, min_var=.1):\n",
    "    \n",
    "    history = []\n",
    "    old_log = None\n",
    "    new_log = None\n",
    "    while old_log is None or (new_log - old_log > 1e-6*np.abs(new_log)):\n",
    "        old_log=new_log\n",
    "        \n",
    "        #E-step\n",
    "        post, new_log = estep(X, mu, p, var)\n",
    "        history.append(new_log)\n",
    "        \n",
    "        #M-step\n",
    "        mu, p, var = mstep(X, post, min_var, mu, p, var)\n",
    "    \n",
    "    return mu, p,var, post, new_log, history\n",
    "        \n",
    "mu, p, var, _, _, history = run(X, mu, p, var, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the Log-Likelihood\n",
    "\n",
    "We will use MatPlotLib to plot the log-likelihood and confirm that our log-likelihood is increasing and shows signs of convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history)\n",
    "plt.ylabel('Log Likelihood')\n",
    "plt.xlabel('EM Algorithm Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Completion\n",
    "\n",
    "Next, we use our mixture to \"fill\" the incomplete matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill(X,mu,p,var):\n",
    "    \n",
    "    post,_ = estep(X,mu,p,var)\n",
    "    X_pred = X.copy()\n",
    "    miss_indices = np.where(X == 0)\n",
    "    X_pred[miss_indices] = (post@mu)[miss_indices]\n",
    "    \n",
    "    return X_pred\n",
    "\n",
    "X_filled = fill(X,mu,p,var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our Model\n",
    "\n",
    "Finally we calculate the rmse between the complete and incomplete matrix. (It is important to know that the completed matrix was not used at all up to this point as in real world applications, we would not have access to that data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE(incomplete matrix, complete matrix) =  1.6787480867863673\n",
      "RMSE(GMM-filled matrix, complete matrix) =  0.48253399427052146\n"
     ]
    }
   ],
   "source": [
    "def rmse(X, Y):\n",
    "    return np.sqrt(np.mean((X - Y)**2))\n",
    "\n",
    "print('RMSE(incomplete matrix, complete matrix) = ', rmse(X, X_complete))\n",
    "print('RMSE(GMM-filled matrix, complete matrix) = ', rmse(X_filled, X_complete))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE is much lower, meaning that our filled matrix is now much closer to the completed matrix, the \"ground truth\", and is a good predictor of how much a viewer would enjoy a movie which they have not previously watched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing to sklearn.cluster.KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us define functions for filling the incomplete matrix with the representatives from the KMeans model. The function $\\texttt{means-matrix}$ creates a matrix $\\texttt{X_means}$ where each sample is replaced with the representative of the cluster is was assigned to. Recall that the representative of a cluster $C$ in KMeans is the mean $\\mu(C)$ of the samples in cluster $C$. However, we do not want to replace the non-zero values as we know those values to be true and doing so would certaining increase the total error. Hence, the function $\\texttt{fill_zeros}$ replaces the zeros in the incomplete matrix with the value from $\\texttt{X_means}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def means_matrix(X_original, model):\n",
    "    \n",
    "    K,d = model.cluster_centers_.shape\n",
    "    \n",
    "    X_new = []\n",
    "    \n",
    "    for i in model.labels_:\n",
    "        \n",
    "        X_new.append(model.cluster_centers_[i])\n",
    "        \n",
    "    return np.array(X_new)\n",
    "        \n",
    "\n",
    "def fill_zeros(X_old, X_means):\n",
    "    \n",
    "    X_pred = X_old.copy()\n",
    "    zeros = np.where(X_pred == 0)\n",
    "    X_pred[zeros] = X_means[zeros]\n",
    "    \n",
    "    return X_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we fit the KMeans model on the incomplete matrix $\\texttt{X}$ and use the functions we just created to fill the incomplete matrix. Finally, we compute the RMSE between this filled matrix and the original complete matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.920752319678606\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=K, random_state=0).fit(X)\n",
    "\n",
    "X_means = means_matrix(X,kmeans)\n",
    "X_pred = fill_zeros(X,X_means)\n",
    "print(rmse(X_pred, X_complete))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our GMM performed better than the KMeans from sklearn.cluster. I encourage the reader to try out different number of clusters by changing $K$ and changing the random seeds. I don't claim that this has not been done before, however, the ability to create your own models from scratch removes the limitation of only being able to use those in popular machine learning libraries. In this case, there are boundless models that could be created with this approach. The obvious one is that we may consider different distributions for the cluster. In particular, I am interested trying not spherical distributions like ellipsoidal distributions and even more general manifolds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
